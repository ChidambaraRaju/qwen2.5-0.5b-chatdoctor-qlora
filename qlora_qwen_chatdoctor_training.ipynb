{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End QLoRA Fine-tuning: Qwen2.5-0.5B-Instruct on ChatDoctor-HealthCareMagic-100k\n",
    "\n",
    "This notebook demonstrates a complete QLoRA (Quantized Low-Rank Adaptation) fine-tuning pipeline for the **Qwen2.5-0.5B-Instruct** model on the **ChatDoctor-HealthCareMagic-100k** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl evaluate rouge_score huggingface_hub wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Weights & Biases for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Login to W&B (this will prompt for API key if not already logged in)\n",
    "wandb.login()\n",
    "\n",
    "# W&B Configuration\n",
    "WANDB_PROJECT = \"qwen-chatdoctor-qlora\"  # Your project name\n",
    "WANDB_RUN_NAME = \"qwen2.5-0.5b-chatdoctor-run1\"  # Descriptive run name\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    name=WANDB_RUN_NAME,\n",
    "    config={\n",
    "        \"model_id\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "        \"dataset\": \"lavita/ChatDoctor-HealthCareMagic-100k\",\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"num_epochs\": 1,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"optimizer\": \"paged_adamw_8bit\",\n",
    "    },\n",
    "    tags=[\"qlora\", \"qwen2.5\", \"medical\", \"chatdoctor\"],\n",
    ")\n",
    "\n",
    "print(f\"W&B Run URL: {wandb.run.get_url()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Constants\n",
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "DATASET_NAME = \"lavita/ChatDoctor-HealthCareMagic-100k\"\n",
    "OUTPUT_DIR = \"./qwen-chatdoctor-qlora\"\n",
    "ADAPTER_NAME = \"qwen2.5-0.5b-chatdoctor-qlora\"  # Name for HuggingFace Hub\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_SEQ_LENGTH = 512\n",
    "TRAIN_SAMPLES = 10000  # Subset for faster training (set to None for full dataset)\n",
    "EVAL_SAMPLES = 500\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 1\n",
    "WARMUP_RATIO = 0.03\n",
    "\n",
    "# LoRA hyperparameters\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Dataset Loading & Preprocessing\n",
    "\n",
    "The ChatDoctor-HealthCareMagic-100k dataset contains medical Q&A conversations. We'll format these into a chat template suitable for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading ChatDoctor-HealthCareMagic-100k dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "print(f\"\\nSample data point:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names and data structure\n",
    "print(\"Column names:\", dataset['train'].column_names)\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    for key, value in dataset['train'][i].items():\n",
    "        print(f\"{key}: {str(value)[:200]}...\" if len(str(value)) > 200 else f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer for chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set padding side to right for causal LM training\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(example):\n",
    "    \"\"\"\n",
    "    Format the dataset examples into Qwen's chat template.\n",
    "    The dataset has 'input' (patient question) and 'output' (doctor response) columns.\n",
    "    \"\"\"\n",
    "    # Create system message for medical context\n",
    "    system_message = \"You are a helpful and professional medical assistant. Provide accurate and empathetic medical advice based on the patient's symptoms and concerns.\"\n",
    "    \n",
    "    # Build the conversation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": example['input']},\n",
    "        {\"role\": \"assistant\", \"content\": example['output']}\n",
    "    ]\n",
    "    \n",
    "    # Apply the chat template\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Test the formatting function\n",
    "sample = dataset['train'][0]\n",
    "formatted_sample = format_chat_template(sample)\n",
    "print(\"Formatted sample:\")\n",
    "print(formatted_sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and eval splits\n",
    "print(\"Preparing dataset splits...\")\n",
    "\n",
    "# Shuffle and select subsets\n",
    "train_dataset = dataset['train'].shuffle(seed=42)\n",
    "\n",
    "if TRAIN_SAMPLES:\n",
    "    train_dataset = train_dataset.select(range(min(TRAIN_SAMPLES, len(train_dataset))))\n",
    "\n",
    "# Split into train and eval\n",
    "train_test_split = train_dataset.train_test_split(test_size=EVAL_SAMPLES, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(format_chat_template, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(format_chat_template, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "print(\"\\nDataset preparation complete!\")\n",
    "print(f\"Sample formatted text length: {len(train_dataset[0]['text'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Configuration with 4-bit Quantization\n",
    "\n",
    "We'll load the Qwen2.5-0.5B-Instruct model with 4-bit quantization using BitsAndBytes for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normalized Float 4-bit\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16 for stability\n",
    "    bnb_4bit_use_double_quant=True,  # Nested quantization for more memory savings\n",
    ")\n",
    "\n",
    "print(\"BitsAndBytes configuration:\")\n",
    "print(f\"  - 4-bit quantization: Enabled\")\n",
    "print(f\"  - Quantization type: NF4 (Normalized Float 4)\")\n",
    "print(f\"  - Compute dtype: bfloat16\")\n",
    "print(f\"  - Double quantization: Enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with quantization\n",
    "print(f\"Loading {MODEL_ID} with 4-bit quantization...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Disable cache for training\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "\n",
    "# Calculate model memory footprint\n",
    "def get_model_memory(model):\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "    return (mem_params + mem_buffers) / 1e6  # Convert to MB\n",
    "\n",
    "print(f\"Approximate model memory: {get_model_memory(model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"Model prepared for QLoRA training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. QLoRA Configuration\n",
    "\n",
    "Configure LoRA adapters that will be trained on top of the frozen quantized base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,                      # Rank of the update matrices\n",
    "    lora_alpha=LORA_ALPHA,          # Scaling factor\n",
    "    lora_dropout=LORA_DROPOUT,      # Dropout probability\n",
    "    bias=\"none\",                    # Bias training strategy\n",
    "    task_type=\"CAUSAL_LM\",          # Task type for the model\n",
    "    target_modules=[                # Modules to apply LoRA to\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  - Rank (r): {LORA_R}\")\n",
    "print(f\"  - Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  - Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  - Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "    print(f\"Total parameters: {all_params:,}\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training with SFTTrainer\n",
    "\n",
    "Use TRL's SFTTrainer for supervised fine-tuning with our configured QLoRA setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments using SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",  # Set to \"wandb\" or \"tensorboard\" for tracking\n",
    "    run_name = WANDB_RUN_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,  # Disable packing for chat format\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Max sequence length: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"SFTTrainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting QLoRA fine-tuning...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Clear CUDA cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"  - Total training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"  - Training samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"  - Training steps per second: {train_result.metrics['train_steps_per_second']:.2f}\")\n",
    "print(f\"  - Final training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained LoRA adapters\n",
    "print(f\"Saving LoRA adapters to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Evaluation with ROUGE Metrics\n",
    "\n",
    "Evaluate the fine-tuned model using ROUGE-1, ROUGE-2, and ROUGE-L metrics to measure the quality of generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ROUGE metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "print(\"ROUGE metric loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the original eval dataset (before formatting) for evaluation\n",
    "print(\"Preparing evaluation data...\")\n",
    "\n",
    "# Reload dataset to get original input/output pairs\n",
    "eval_raw_dataset = load_dataset(DATASET_NAME, split='train')\n",
    "eval_raw_dataset = eval_raw_dataset.shuffle(seed=42).select(range(TRAIN_SAMPLES if TRAIN_SAMPLES else len(eval_raw_dataset)))\n",
    "eval_raw_split = eval_raw_dataset.train_test_split(test_size=EVAL_SAMPLES, seed=42)\n",
    "eval_raw_dataset = eval_raw_split['test']\n",
    "\n",
    "# Use a smaller subset for faster evaluation\n",
    "EVAL_SUBSET_SIZE = 100  # Evaluate on 100 samples for speed\n",
    "eval_subset = eval_raw_dataset.select(range(min(EVAL_SUBSET_SIZE, len(eval_raw_dataset))))\n",
    "\n",
    "print(f\"Evaluation subset size: {len(eval_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, input_text, max_new_tokens=256):\n",
    "    \"\"\"\n",
    "    Generate a response from the model for a given input.\n",
    "    \"\"\"\n",
    "    # Create system message\n",
    "    system_message = \"You are a helpful and professional medical assistant. Provide accurate and empathetic medical advice based on the patient's symptoms and concerns.\"\n",
    "    \n",
    "    # Build the conversation (without assistant response for generation)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": input_text},\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template with generation prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions for ROUGE evaluation...\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in tqdm(eval_subset, desc=\"Evaluating\"):\n",
    "    # Generate response\n",
    "    prediction = generate_response(model, tokenizer, example['input'])\n",
    "    predictions.append(prediction)\n",
    "    references.append(example['output'])\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions)} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few example predictions vs references\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample Predictions vs References\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(min(3, len(predictions))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"\\nInput: {eval_subset[i]['input'][:300]}...\")\n",
    "    print(f\"\\nReference: {references[i][:300]}...\")\n",
    "    print(f\"\\nPrediction: {predictions[i][:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE scores\n",
    "print(\"Computing ROUGE metrics...\")\n",
    "\n",
    "rouge_results = rouge.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    use_aggregator=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ROUGE Evaluation Results (Fine-tuned Model)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
    "print(f\"ROUGE-Lsum: {rouge_results['rougeLsum']:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with base model performance\n",
    "# This helps demonstrate the improvement from fine-tuning\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Loading base model for comparison...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load base model (without LoRA)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "base_model.eval()\n",
    "\n",
    "# Generate predictions with base model\n",
    "print(\"Generating predictions with base model...\")\n",
    "\n",
    "base_predictions = []\n",
    "for example in tqdm(eval_subset, desc=\"Base Model Evaluation\"):\n",
    "    prediction = generate_response(base_model, tokenizer, example['input'])\n",
    "    base_predictions.append(prediction)\n",
    "\n",
    "# Compute ROUGE for base model\n",
    "base_rouge_results = rouge.compute(\n",
    "    predictions=base_predictions,\n",
    "    references=references,\n",
    "    use_aggregator=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ROUGE Evaluation Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<15} {'Base Model':<15} {'Fine-tuned':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "    base_score = base_rouge_results[metric]\n",
    "    ft_score = rouge_results[metric]\n",
    "    improvement = ft_score - base_score\n",
    "    print(f\"{metric.upper():<15} {base_score:<15.4f} {ft_score:<15.4f} {improvement:+.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Clean up base model\n",
    "del base_model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Push Adapters to HuggingFace Hub\n",
    "\n",
    "Upload the trained LoRA adapters to your HuggingFace account for sharing and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace Hub\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your HuggingFace username\n",
    "api = HfApi()\n",
    "user_info = api.whoami()\n",
    "username = user_info['name']\n",
    "\n",
    "print(f\"Logged in as: {username}\")\n",
    "\n",
    "# Define the repository name\n",
    "repo_name = f\"{username}/{ADAPTER_NAME}\"\n",
    "print(f\"Repository name: {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model card content\n",
    "model_card = f\"\"\"---\n",
    "base_model: {MODEL_ID}\n",
    "library_name: peft\n",
    "license: apache-2.0\n",
    "tags:\n",
    "- qwen2\n",
    "- qlora\n",
    "- medical\n",
    "- healthcare\n",
    "- chatdoctor\n",
    "- peft\n",
    "- lora\n",
    "datasets:\n",
    "- lavita/ChatDoctor-HealthCareMagic-100k\n",
    "language:\n",
    "- en\n",
    "---\n",
    "\n",
    "# {ADAPTER_NAME}\n",
    "\n",
    "This is a QLoRA fine-tuned adapter for **{MODEL_ID}** trained on the **ChatDoctor-HealthCareMagic-100k** medical Q&A dataset.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model**: {MODEL_ID}\n",
    "- **Fine-tuning Method**: QLoRA (4-bit quantization + LoRA)\n",
    "- **Dataset**: [ChatDoctor-HealthCareMagic-100k](https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k)\n",
    "- **Training Samples**: {len(train_dataset):,}\n",
    "- **Evaluation Samples**: {len(eval_dataset):,}\n",
    "\n",
    "## LoRA Configuration\n",
    "\n",
    "- **Rank (r)**: {LORA_R}\n",
    "- **Alpha**: {LORA_ALPHA}\n",
    "- **Dropout**: {LORA_DROPOUT}\n",
    "- **Target Modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "- **Epochs**: {NUM_EPOCHS}\n",
    "- **Batch Size**: {BATCH_SIZE}\n",
    "- **Gradient Accumulation Steps**: {GRADIENT_ACCUMULATION_STEPS}\n",
    "- **Learning Rate**: {LEARNING_RATE}\n",
    "- **Max Sequence Length**: {MAX_SEQ_LENGTH}\n",
    "\n",
    "## Evaluation Results (ROUGE Metrics)\n",
    "\n",
    "| Metric | Score |\n",
    "|--------|-------|\n",
    "| ROUGE-1 | {rouge_results['rouge1']:.4f} |\n",
    "| ROUGE-2 | {rouge_results['rouge2']:.4f} |\n",
    "| ROUGE-L | {rouge_results['rougeL']:.4f} |\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"{MODEL_ID}\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{MODEL_ID}\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"{repo_name}\")\n",
    "\n",
    "# Generate response\n",
    "messages = [\n",
    "    {{\"role\": \"system\", \"content\": \"You are a helpful medical assistant.\"}},\n",
    "    {{\"role\": \"user\", \"content\": \"What are the symptoms of diabetes?\"}},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "## Training Framework\n",
    "\n",
    "- **transformers**: HuggingFace Transformers\n",
    "- **peft**: Parameter-Efficient Fine-Tuning\n",
    "- **trl**: Transformer Reinforcement Learning (SFTTrainer)\n",
    "- **bitsandbytes**: 4-bit quantization\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "This model is for educational and research purposes only. It should not be used as a substitute for professional medical advice, diagnosis, or treatment. Always seek the advice of a qualified healthcare provider.\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(os.path.join(OUTPUT_DIR, \"README.md\"), \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"Model card created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the model to HuggingFace Hub\n",
    "print(f\"Pushing adapters to HuggingFace Hub: {repo_name}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Push model and tokenizer\n",
    "model.push_to_hub(\n",
    "    repo_name,\n",
    "    use_auth_token=True,\n",
    "    commit_message=\"Upload QLoRA adapters for Qwen2.5-0.5B-Instruct fine-tuned on ChatDoctor\"\n",
    ")\n",
    "\n",
    "tokenizer.push_to_hub(\n",
    "    repo_name,\n",
    "    use_auth_token=True,\n",
    "    commit_message=\"Upload tokenizer\"\n",
    ")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"SUCCESS! Adapters pushed to HuggingFace Hub!\")\n",
    "print(f\"Repository URL: https://huggingface.co/{repo_name}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
